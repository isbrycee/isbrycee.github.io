<!DOCTYPE HTML>

<style>
  #full {
    display: none;
    font-size: 100px;
  }
</style>

<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jing HAO</title>
	
  <meta name="author" content="Jing Hao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/me.png">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:30%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jing HAO</name>
              </p>
		    
              <p>
	      I'm pursuing Ph.D. degree in the University of HongKong, Faculty of Dentistry (Ranking 3rd in the world), supervised by <a href="https://scholar.google.com/citations?user=17V5x14AAAAJ&hl=zh-CN">Prof. Kuo Feng Hung</a> and <a href="https://scholar.google.com/citations?user=mXzEprAAAAAJ">Prof. Tsoi, James Kit Hon</a>. 
              </p>
	      <p>
	      Previously, I worked as a Computer Vision Engineer on Baidu VIS from 2022.07 to 2024.08. I received my M.S. degree in Huazhong University of Science and Technology (HUST, 2022), and B.S. degree in Chinese University of Mining and Technology (CUMT, 2020).
	      </p>
              <p>
              My research interests span the area of computer vision, self-supervised pre-training, multimodal large language model (mllm), and AI4Science.
              </p>
		    
              <br>&#x1F463 Below are my detailed educational informations:<br>
		<li style="margin: 5px;" >
		  2024.09 - now, Ph.D. Student, Faculty of Dentistry (Ranking 3rd in the world), The University of HongKong.
		</li>
		<li style="margin: 5px;" >
		  2022.06 - 2024.08, worked on Baidu VIS as an Computer Vision Engineer. 
		</li>
		<li style="margin: 5px;" >
		  2020.06 - 2022.06, M.Eng., School of Mechanical Science and Engineering, Huazhong University of Science and Technology (HUST).
		</li>
		<li style="margin: 5px;" >
		  2016.09 - 2020.06, B.Eng., School of Mechatronic Engneering, Chinese University of Mining and Technology (CUMT).
		</li>
		    
              <p style="text-align:center">
                <a href="mailto:isjinghao@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=E8R8c00AAAAJ&hl=zh-CN"> Google Scholar</a> &nbsp/&nbsp
                <!-- <a href=""> WeChat </a>&nbsp/&nbsp -->
                <a href="https://mp.weixin.qq.com/s/Nc15If10h9tED3hL92JW3A">Personal WeChat Platform</a> 
              </p>
            </td>
            <!-- <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/rym.jpg">
            </td> -->
          </tr>
        </tbody>
	</table>
	      
	<!-- self-intro -->
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:73%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jing HAO</name>
              </p>
              <p>
	      I'm pursuing Ph.D. degree in the University of HongKong, Faculty of Dentistry (Ranking 3rd in the world), supervised by <a href="https://scholar.google.com/citations?user=17V5x14AAAAJ&hl=zh-CN">Prof. Kuo Feng Hung</a> and <a href="https://scholar.google.com/citations?user=mXzEprAAAAAJ">Prof. Tsoi, James Kit Hon</a>. 
              </p>
	      <p>
	      Previously, I worked as a Computer Vision Engineer on Baidu VIS from 2022.07 to 2024.08. I received my M.S. degree in Huazhong University of Science and Technology (HUST, 2022), and B.S. degree in Chinese University of Mining and Technology (CUMT, 2020).
	      </p>
              <p>
              My research interests span the area of computer vision, self-supervised pre-training, multimodal large language model (mllm), and AI4Science.
              </p>
              <p style="text-align:center;font-size:20px;">		      
                <a href="mailto:isjinghao@gmail.com" target="_blank"><button style='font-size:22px'>E-Mail <i class='fas fa-envelope'></i> </a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=E8R8c00AAAAJ&hl=zh-CN" target="_blank"><i class="ai ai-google-scholar" style="font-size:20px;"></i></a> &nbsp/&nbsp
                <a href="https://github.com/isbrycee/" target="_blank"><i class="fab fa-github" style="font-size:20px;"></i></a>&nbsp/&nbsp
		<a href="https://mp.weixin.qq.com/s/Nc15If10h9tED3hL92JW3A" target="_blank"><i class="fa-brands fa-weixin" style="font-size:20px;"></i></a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="static/images/profile.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    </tr>
  </tbody></table>
<!-- end self-intro -->
	
<!-- News -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" > 
		[July.2024] I got the firm offer from HKU, and had been an formal Ph.D. cadicate ! Let's start exploring the world.
              </li>
	      <li style="margin: 5px;" > 
		[July.2024] One paper <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608024004301">METR</a> has been accepted to <b>Neural Networks</b>b> !
              </li>
	      <li style="margin: 5px;" >
                [Aug.2023] I have received my IELTS scores 7(6)!
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                See full list at <a href="https://scholar.google.com/citations?user=E8R8c00AAAAJ&hl=zh-CN">Google Scholar.</a> (* indicates equal contribution, # indicates corresponding author)
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/framework_METR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2304.03580">
              <papertitle>Language-aware Multiple Datasets Detection Pretraining for DETRs</papertitle>
              </a>
              <br>
              <strong>Jing Hao*</strong>,
              Song Chen*,
              Xiaodi Wang,
              Shumin Han
              <!-- <a href="http://www.lsl.zone/"> Shilong Liu*</a>, -->
              <br>
              <em>ArXiv, 2023</em>
              <br>
              <!-- <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2306.07265">paper</a> /
                <a href="https://rentainhe.github.io/projects/detrex/">project</a> /
                <a href="https://github.com/IDEA-Research/detrex">code</a> /
                <a href="https://zhuanlan.zhihu.com/p/571307593">introduction (in Chinese)</a>
              </div> -->
              <p>A strong framework for utilizing Multiple datasets to pretrain DETR-like detectors without the need for manual label spaces integration.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/framework_GSCA.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openreview.net/pdf?id=isodM5jTA7h">
              <papertitle>Simple Parameter-free Self-attention Approximation</papertitle>
              </a>
              <br>
              YuwenZHai*,
              <strong>Jing Hao*</strong>,
              Liang Gao,
              Xinyu Li,
              Yiping Gao,
              Shumin Han
              <br>
              <em>ICLR Tiny Paper, 2023</em>
              <br>
              <!-- <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2305.15023">paper</a> /
                <a href="https://github.com/luogen1996/LaVIN">code</a>
              </div> -->
              <p>A self-attention approximation without training parameters which captures global spatial features with linear complexity.</p>
              <!-- <a href="https://arxiv.org/pdf/2305.15023.pdf">[arXiv]</a>
              <a href="https://github.com/luogen1996/LaVIN">[Code]</a> -->
              <!-- <a href="https://zhuanlan.zhihu.com/p/422838496">[中文解读]</a> -->
              <!-- <img alt="Github stars" src="https://github.com/luogen1996/LaVIN?style=social"> -->
              <!-- <p> The first end-to-end efficient large vision-language instructed model.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/framework_ANAFNet.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="">
              <papertitle>A Stronger Stitching Algorithm for Fisheye Images based on Deblurring and Registration</papertitle>
              </a>
              <br>
              <strong>Jing Hao*</strong>, 
              Jingming Xie*, 
              Jinyuan Zhang, 
              Moyun Liu
              <br>
              <em>IEEE Sensors Letters</em>
              <!-- <br>
              <font color="#FF0000">The Strongest Detector under 1B Parameters. Rank Top-2 on COCO Leaderboard (2023.04)</font>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.13027">paper</a> /
                <a href="https://github.com/IDEA-Research/Stable-DINO">code</a>
              </div> -->
              <p>A stronger stitching algorithm for fisheye images by combining the traditional image processing method with deep learning.</p>
              <!-- <a href="https://arxiv.org/abs/2304.13027">[arXiv]</a>
              <a href="https://github.com/microsoft/FocalNet">[FocalNet]</a>
              <a href="https://github.com/IDEA-Research/Stable-DINO">[Stable-DINO]</a>
              <img alt="Github stars" src="https://img.shields.io/github/stars/microsoft/FocalNet?style=social"> -->
              <!-- <p> We proposed <b>Focal-Stable-DINO</b> by combining strong FocalNet backbone and effective Stable-DINO detector.
                Remarkably, <b>without any test time augmentation</b>, our model achieves <b>64.6 AP</b> on COCO val2017 and <b>64.8 AP</b> on COCO test-dev with only publicly avaliable datasets for training.
              </p> -->
              <!-- <p> Combine FocalNet-Huge and Stable-DINO, we achieved <b>64.6 AP</b> on COCO val2017 and <b>64.8 AP</b> on COCO test-dev <b>without any test time augmentation</b>. 
              </p> -->
              <!-- <p> <i> "Like a <a href="https://github.com/zzyunzhi/object-intrinsics">rose</a>, each person can unfold their own unique beauty. May we witness a flourishing of varied approaches and techniques in this field." </i> 
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/framework_X_ray.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/pdf/2110.09278.pdf">
              <papertitle>A Lightweight and Accurate Recognition Framework for Signs of X-ray Weld Images</papertitle>
              </a>
              <br>
              Moyun Liu,
              Jingming Xie,
              <strong>Jing Hao</strong>, 
              Yang Zhang, 
              Xuzhan Chen, 
              Youping Chen
              <br>
              <em>Computers in Industry, 2022</em>
              <br>
              <!-- <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.04742">paper</a> /
                <a href="https://github.com/IDEA-Research/Stable-DINO">code</a>
              </div> -->
              <p>A signs recognition framework based on convolutional neural networks (CNNs) for weld images.</p>
              <!-- <a href="https://arxiv.org/abs/2304.04742">[arXiv]</a>
              <a href="https://github.com/IDEA-Research/Stable-DINO">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/IDEA-Research/StableDINO?style=social"> -->
              <!-- <p> We propose two simple yet effective
                modifications by integrating positional metrics to DETR’s
                classification loss and matching cost, named positionsupervised loss and position-modulated cost to solve the unstable
                matching problem. And our Stable-DINO achieves <b>63.8AP</b> on COCO test-dev with a Swin-Large backbone.
              </p> -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p><heading>Awards & Honors</heading></p>
            <p>
                <li style="margin: 5px;" >
                  <b><font color="#FF0000">National Scholarship</font></b> (2018)
                </li>
                <li style="margin: 5px;" >
                  First-class Scholarship of HUST (2021)
                </li>
                <li style="margin: 5px;" >
                  First-class Scholarship of CUMT (2019)
                </li>
                <li style="margin: 5px;" >
                  Second-class Scholarship of CUMT (2017)
                </li>
                <li style="margin: 5px;" >
                  Outstanding student of CUMT (2018)
                </li>
                <li style="margin: 5px;" >
                  Excellent Student Leader of Jiangsu Province (2019)
                </li>
                <li style="margin: 5px;" >
                  Excellent Young Volunteer of Xuzhou City (2019)
                </li>
              </p>
            <!-- <p>
               * indicates project lead, # indicates directional lead
            </p> -->
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script> -->

      <!-- <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (<strong>ICDE</strong>)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr></tbody></table>
      </div> -->


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 1st place in the MVP Point Cloud Completion Challenge (ICCV 2021 Workshop)</li>
                <li style="margin: 5px;"> Baidu Top 100 Chinese Rising Stars in AI <a href="https://mp.weixin.qq.com/s/v7ITiZXOJiDUbPlRlcqQRA">(百度AI华人新星百强榜)</a></li>
                <li style="margin: 5px;"> CVPR 2021 Outstanding Reviewer</li>
                <li style="margin: 5px;"> ECCV 2020 Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020 Workshop)</li>
                <li style="margin: 5px;"> 2019 CCF-CV Academic Emerging Award (CCF-CV 学术新锐奖)</li>
                <li style="margin: 5px;"> 2019 Chinese National Scholarship </li>
                <li style="margin: 5px;"> ICME 2019 Best Reviewer Award </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
                <li style="margin: 5px;"> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
                <li style="margin: 5px;"> 1st place in Momenta Lane Detection Challenge </li>
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> Tutorial on Deep Reinforcement Learning for Computer Vision at CVPR 2019 <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2018-2022, ICCV 2019-2021, ECCV 2020-2022, NeurIPS 2019-2022, ICML 2019-2022, ICLR 2021-2023, SIGGRAPH Asia 2022, AAAI 2020-2022, WACV 2020-2022, ICME 2019-2022, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, IJCV, T-NNLS, T-IP, T-MM, Pattern Recognition
              </li>
            </p>
          </td>
        </tr>
      </tbody></table> -->
       
  
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table> -->
 
<!-- <p><center>
	  <div id="clustrmaps-widget" style="width:5%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yp3s8rdiQW_pbzmBOzWDx2Fv6afIlEpV-k1EZiYIkEY"></script>
	  </div>        
	  <br>
	    &copy; Tianhe Ren | Last updated: Nov 9, 2022
</center></p>
</body> -->

</html>
